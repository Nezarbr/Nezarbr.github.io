{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/my-first-article/","result":{"data":{"site":{"siteMetadata":{"name":"Nezar Boujida","title":"Nezar Boujida | Data Scientist","description":null,"about":"Hello, I'm Nezar. I'm currently studying Applied Mathematics and Computer Science at Sorbonne University, where I dive into data science, explore datasets, and uncover meaningful patterns to solve real-world problems. I've gained experience as a Data Scientist intern at Datategy and Hiflow, and now I'm working as an Applied Scientist at Launchmetrics. In my free time, I enjoy long-distance running, traveling, and discovering new places, always seeking out new challenges to grow and learn.","author":null,"github":"https://github.com/Nezarbr","linkedin":"https://www.linkedin.com/in/nezar-boujida/"}},"markdownRemark":{"id":"ed16b32f-10f2-524a-8e62-16056ee536c9","excerpt":"Maximum Likelihood and Loss Functions: From Probability to Optimization In machine learning, we often take loss functions for granted. Data scientists routinely…","html":"<h2>Maximum Likelihood and Loss Functions: From Probability to Optimization</h2>\n<p>In machine learning, we often take loss functions for granted. Data scientists routinely use mean squared error for regression tasks and cross-entropy for classification problems, but have you ever wondered why these particular functions are chosen? Far from being arbitrary choices, these loss functions emerge naturally from fundamental principles of probability theory.</p>\n<p>At the heart of this connection lies Maximum Likelihood Estimation (MLE), a powerful statistical principle that bridges the gap between probability theory and optimization. When we train a machine learning model by minimizing a loss function, we’re often unknowingly performing maximum likelihood estimation under specific probabilistic assumptions about our data.</p>\n<p>Understanding this connection is more than just theoretical elegance. It provides practitioners with:</p>\n<ul>\n<li>Insight into when standard loss functions are appropriate and when they might need modification</li>\n<li>A deeper understanding of model assumptions and limitations</li>\n<li>Tools for better model interpretation and debugging</li>\n</ul>\n<p>In this article, we’ll uncover how commonly used loss functions naturally arise from maximum likelihood estimation. We’ll walk through the derivation of both linear regression’s mean squared error and logistic regression’s cross-entropy loss, showing how they emerge from simple probabilistic assumptions about the data generation process.</p>\n<hr>\n<h2>Maximum Likelihood Estimation</h2>\n<h3>Concept</h3>\n<p>Maximum Likelihood Estimation (MLE) is a fundamental method for finding the parameters of a probability distribution that best explains observed data. The key idea is simple: given a known type of distribution (like normal, exponential, etc.), MLE finds the specific parameters of that distribution that make our observed data most probable.</p>\n<p>Think of it as “reverse engineering” the parameters: we see the data, we know the type of distribution that generated it, and we want to find the exact parameters that were most likely used to generate that data.</p>\n<h3>Mathematical Foundation</h3>\n<h4>Basic Formula</h4>\n<p>Given independent observations (x<em>1, x</em>2, \\dots, x<em>n) from a known distribution type, the likelihood function is:\n[\nL(\\theta) = P(x</em>1, x<em>2, \\dots, x</em>n | \\theta) = P(x<em>1 | \\theta) \\times P(x</em>2 | \\theta) \\times \\dots \\times P(x_n | \\theta)\n]\nwhere:</p>\n<ul>\n<li>(\\theta) represents the parameters we want to estimate.</li>\n<li>The distribution type is known (e.g., normal, exponential).</li>\n<li>(P(x<em>i | \\theta)) is the probability of observing (x</em>i) given parameters (\\theta).</li>\n</ul>\n<h3>Understanding MLE Through Visualization</h3>\n<div style=\"text-align: center;\">\n    <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; margin: 0 0 30px;\">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/00bc6b9b81ed01658cf4d0d80f1ee739/e1250/distrib.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB4klEQVQoz22SW5OaQBCF+f+/J5cqI3JRcRU1WWuvyWpWlMvMMAwXZWF9OGlATSq1D81ATfc5X3ej1fU7jsf6g6gQhgJhwOEHDPt9RN/du1I5yrKrK8saaXFEkmSoqndozSOKUorsg5MiSPC82WLrCcS8AOc5GMuveYxlWP/yMZ/fkaGE9vZWt0lC5NezKdj7BdbrFMvVI3TXxuLHLcTGh9hxcBISovhbx1IEZJwkB2h1fWpdmkvOu3PrZZBxidXDM3TbxWSyQO/rZ9w9PkGKA9g2OpOe60i8gUjTElpB/f/r5gddxHGB6XKBgTGD/sXEUDcwns8RJ5QfSAg/boU6kE48TY/Q8vzQiZ1n4+1IjBKf1huYwwmcgQn9Ux99EnbdBV62O5qVurbOxX+Cl5YbuqZVzo949QJYjoPhcIpxT4dhjtDrzzCyHaL+jpfXPWIyZ150pWTRWfCyFD8oyJnakTmW9/ewbKLrmxhRWGMHunGDXu+G5ulivrpFEEkoAonPrXeE5eW3ybDbSSLzMaO5mQMLY92iudmwjBEMe9Qa6LqDb/0piRP91MXDz9/wPdaKhWHaEZ5OJzSUKpW0QYYkjKA4py1LxFJBJQoRY1RAdzKhZSVElLW/ScBiCKXa+iaqqsYfipTc3NrgPhAAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Distribution Comparison with Typical Observations\" title=\"Distribution Comparison with Typical Observations\" src=\"/static/00bc6b9b81ed01658cf4d0d80f1ee739/fcda8/distrib.png\" srcset=\"/static/00bc6b9b81ed01658cf4d0d80f1ee739/12f09/distrib.png 148w,\n/static/00bc6b9b81ed01658cf4d0d80f1ee739/e4a3f/distrib.png 295w,\n/static/00bc6b9b81ed01658cf4d0d80f1ee739/fcda8/distrib.png 590w,\n/static/00bc6b9b81ed01658cf4d0d80f1ee739/efc66/distrib.png 885w,\n/static/00bc6b9b81ed01658cf4d0d80f1ee739/c83ae/distrib.png 1180w,\n/static/00bc6b9b81ed01658cf4d0d80f1ee739/e1250/distrib.png 1786w\" sizes=\"(max-width: 590px) 100vw, 590px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</div>\n<h4>Example Setup</h4>\n<p>In this example, we have data that follows a normal distribution with unknown parameters to be estimated. While the true parameters are (\\mu=5) and (\\sigma=1) (known here for demonstration), in practice these would be unknown. Let’s compare three possible parameter choices, (\\mu=5), (\\mu=4), and (\\mu=6), all with the same standard deviation (\\sigma=1).</p>\n<p>The gray histogram displays our observed data, which follows a normal distribution with (\\mu=5) and (\\sigma=1). Let’s analyze the likelihood calculation for 5 representative points (4.3, 4.8, 5.0, 5.2, and 5.7):</p>\n<h5>Likelihood Values:</h5>\n<ul>\n<li>(L(5) = 0.341 \\times 0.391 \\times 0.399 \\times 0.391 \\times 0.317 = 0.00532) (true parameters)</li>\n<li>(L(4) = 0.386 \\times 0.311 \\times 0.242 \\times 0.179 \\times 0.066 = 0.00031)</li>\n<li>(L(6) = 0.066 \\times 0.179 \\times 0.242 \\times 0.311 \\times 0.386 = 0.00031)</li>\n</ul>\n<p>The maximum likelihood occurs at (\\mu=5) because:</p>\n<ol>\n<li>For a normal distribution, 68.27% of data points fall between (\\mu \\pm \\sigma) (4 to 6 in our case).</li>\n<li>Our observed points mostly fall in this high-probability region.</li>\n<li>The true distribution ((\\mu=5)) assigns consistently high probabilities to these typical values.</li>\n<li>Alternative distributions ((\\mu=4) or (\\mu=6)) must assign very low probabilities to some points, leading to much lower likelihood values.</li>\n</ol>\n<p>This demonstrates how MLE naturally recovers the true parameters by aligning the distribution’s high-probability region with the actual concentration of data points.</p>\n<hr>\n<h2>From MLE to Loss Functions</h2>\n<p>When we maximize likelihood, we can equivalently minimize its negative logarithm. This transformation turns our probability maximization into a minimization problem:\n[\n\\arg\\max<em>\\theta L(\\theta) = \\arg\\min</em>\\theta -\\log(L(\\theta))\n]</p>\n<hr>\n<h2>Linear Regression: Deriving Mean Squared Error from Maximum Likelihood</h2>\n<h3>Model Assumption:</h3>\n<ul>\n<li>(y = wx + b + \\epsilon), where (\\epsilon) is the error term.</li>\n<li>((y - wx - b)) follows (N(0, \\sigma^2)).</li>\n<li>This means our errors (residuals) follow a normal distribution.</li>\n</ul>\n<h3>Probabilistic Interpretation:</h3>\n<p>For each residual ((y<em>i - wx</em>i - b)):\n[\nP(y<em>i - wx</em>i - b) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y<em>i - wx</em>i - b)^2}{2\\sigma^2}\\right)\n]</p>\n<p>For (n) independent observations:\n[\nL(w, b) = \\prod<em>{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y</em>i - wx_i - b)^2}{2\\sigma^2}\\right)\n]</p>\n<p>Taking the negative logarithm:\n[\n-\\log(L) = \\frac{n}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum (y<em>i - wx</em>i - b)^2\n]</p>\n<p>Since constants ((\\frac{n}{2} \\log(2\\pi\\sigma^2)) and (\\frac{1}{2\\sigma^2})) do not affect optimization:\n[\n-\\log(L) \\propto \\sum (y<em>i - wx</em>i - b)^2\n]</p>\n<p>This is the mean squared error (MSE).</p>\n<hr>\n<h2>Logistic Regression: Deriving Cross-Entropy Loss from Maximum Likelihood</h2>\n<h3>Model Assumption:</h3>\n<ul>\n<li>Binary classification: (y_i \\in {0,1}).</li>\n<li>(P(y=1|x) = \\sigma(wx + b)), where (\\sigma(z) = \\frac{1}{1 + e^{-z}}).</li>\n<li>Outputs follow a Bernoulli distribution.</li>\n</ul>\n<h3>Probabilistic Interpretation:</h3>\n<p>For each observation ((x<em>i, y</em>i)):\n[\nP(y<em>i | x</em>i) = \\sigma(wx<em>i + b)^{y</em>i} (1 - \\sigma(wx<em>i + b))^{1-y</em>i}\n]</p>\n<p>For (n) independent observations:\n[\nL(w, b) = \\prod<em>{i=1}^{n} \\sigma(wx</em>i + b)^{y<em>i} (1 - \\sigma(wx</em>i + b))^{1-y_i}\n]</p>\n<p>Taking the negative logarithm:\n[\n-\\log(L) = -\\sum<em>{i=1}^{n} \\left[ y</em>i \\log(\\sigma(wx<em>i + b)) + (1-y</em>i) \\log(1 - \\sigma(wx_i + b)) \\right]\n]</p>\n<p>This is the binary cross-entropy loss function.</p>\n<hr>","frontmatter":{"title":"Maximum Likelihood and Loss Functions: From Probability to Optimization","date":"November 19, 2024","description":"Explore the connection between probability theory and optimization in machine learning."}}},"pageContext":{"slug":"/blog/my-first-article/","previous":{"fields":{"slug":"/blog/my-fourth-blog/"},"frontmatter":{"title":"Optimizing Automotive Logistics Enhancing Efficiency in Vehicle Transportation Services"}},"next":null}},"staticQueryHashes":["63159454"]}